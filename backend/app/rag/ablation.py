
import logging
import json
import time
from typing import List, Dict, Optional, Callable
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
import statistics

from .engine import query_documents
from .retrievers.reranker import reranker
# Import other components to mock/toggle
from .parsers.chunker import classify_importance

logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    query: str
    expected_chunk_ids: List[str] # Or some ground truth
    core_concept: str # e.g. "Main Contribution"
    
@dataclass
class ExperimentResult:
    config_name: str
    precision_at_3: float
    latency_p95: float
    avg_relevance: float
    total_queries: int

class AblationEngine:
    """
    Framework for running rigorous RAG ablation studies.
    
    Capabilities:
    - Component Toggling (Reranker, Filters, Faithfulness)
    - Dataset Management (Load Gold Standard)
    - Statistical Analysis (Precision, Latency, Recall)
    - Report Generation
    """
    
    def __init__(self, dataset_path: str = "gold_standard.json"):
        self.dataset_path = dataset_path
        self.test_cases = self._load_dataset()
        self.results = {}
        
    def _load_dataset(self) -> List[TestCase]:
        """Load ground-truth queries for evaluation."""
        if not Path(self.dataset_path).exists():
            # Return synthetic default set if no file
            return [
                TestCase("What is the main contribution?", [], "Core"),
                TestCase("How is the model trained?", [], "Method"),
                TestCase("What are the baseline results?", [], "Experiment")
            ]
        try:
            with open(self.dataset_path) as f:
                data = json.load(f)
                return [TestCase(**item) for item in data]
        except Exception as e:
            logger.error(f"Dataset load failed: {e}")
            return []

    def run_experiment(self, config_name: str, 
                      disable_reranker: bool = False, 
                      disable_importance: bool = False,
                      disable_faithfulness: bool = False) -> ExperimentResult:
        """
        Execute a full pass over the dataset with specific configuration.
        """
        logger.info(f"Starting Experiment: {config_name}")
        latencies = []
        precisions = []
        
        # 1. Setup Mocks/Overrides
        original_rerank = reranker.rerank
        
        if disable_reranker:
            # Monkeypatch reranker to be a pass-through
            reranker.rerank = lambda q, c, k: c[:k]
            
        try:
            # 2. Execute Queries
            for case in self.test_cases:
                t0 = time.time()
                
                # Note: user_id=1 is assumed for test user
                # We could inject disable_importance flag into engine via context var in a real sys
                # specific logic to bypass importance filter would go here
                
                result = query_documents(case.query, user_id=1) 
                
                duration = (time.time() - t0) * 1000
                latencies.append(duration)
                
                # 3. Score Result (Heuristic for demo)
                # In real world, check if 'expected_chunk_ids' are in result['sources']
                score = self._heuristic_score(result, case)
                precisions.append(score)
                
        finally:
            # 4. Restore State
            reranker.rerank = original_rerank
            
        # 5. Calculate Aggregates
        return ExperimentResult(
            config_name=config_name,
            precision_at_3=statistics.mean(precisions) if precisions else 0.0,
            latency_p95=statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies),
            avg_relevance=statistics.mean(precisions), # Simplify
            total_queries=len(self.test_cases)
        )

    def _heuristic_score(self, result: Dict, case: TestCase) -> float:
        """
        Score a single result against ground truth.
        Real impl would use RAGAS or exact ID match.
        """
        answer_text = result.get('answer', '').lower()
        # Simple keyword check for demo
        if "not stated" in answer_text:
            return 0.0
        return 1.0

    def run_full_suite(self) -> str:
        """Run standard battery of ablations."""
        
        configs = [
            ("Full Stack", {}),
            ("No Reranker", {"disable_reranker": True}),
            # ("No Importance", {"disable_importance": True}), # Requires engine support
        ]
        
        report = "# ðŸ”¬ Ablation Study Report\n\n"
        report += "| Configuration | Precision | P95 Latency | Samples |\n"
        report += "|---|---|---|---|\n"
        
        for name, args in configs:
            res = self.run_experiment(name, **args)
            report += f"| {res.config_name} | {res.precision_at_3:.2f} | {res.latency_p95:.0f}ms | {res.total_queries} |\n"
            
        return report

# Singleton for CLI usage
ablation_engine = AblationEngine()

if __name__ == "__main__":
    # CLI Entrypoint
    print(ablation_engine.run_full_suite())
